{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:27.877889Z","iopub.status.busy":"2024-04-02T10:23:27.877613Z","iopub.status.idle":"2024-04-02T10:23:42.138180Z","shell.execute_reply":"2024-04-02T10:23:42.136972Z","shell.execute_reply.started":"2024-04-02T10:23:27.877866Z"},"trusted":true},"outputs":[],"source":["!pip install multidict -q #efficientnet_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:42.140597Z","iopub.status.busy":"2024-04-02T10:23:42.140299Z","iopub.status.idle":"2024-04-02T10:23:59.094282Z","shell.execute_reply":"2024-04-02T10:23:59.093434Z","shell.execute_reply.started":"2024-04-02T10:23:42.140570Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","os.environ['WANDB_API_KEY'] = 'cb2feee46a2bfa33da14728baf59ccb43834ec13'\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# from efficientnet_pytorch import EfficientNet\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torchvision.models as models\n","\n","from numba import jit\n","\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm.auto import tqdm\n","from multidict import MultiDict\n","\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.095985Z","iopub.status.busy":"2024-04-02T10:23:59.095503Z","iopub.status.idle":"2024-04-02T10:23:59.106639Z","shell.execute_reply":"2024-04-02T10:23:59.105699Z","shell.execute_reply.started":"2024-04-02T10:23:59.095956Z"},"trusted":true},"outputs":[],"source":["DEFAULT_RANDOM_SEED = 42\n","import random\n","import numpy as np\n","\n","\n","def set_all_seeds(seed=DEFAULT_RANDOM_SEED):\n","\n","    # python's seeds\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","    # torch's seeds\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_all_seeds(seed=DEFAULT_RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.109795Z","iopub.status.busy":"2024-04-02T10:23:59.109493Z","iopub.status.idle":"2024-04-02T10:23:59.116021Z","shell.execute_reply":"2024-04-02T10:23:59.115008Z","shell.execute_reply.started":"2024-04-02T10:23:59.109769Z"},"trusted":true},"outputs":[],"source":["# для получения label для каждого файла\n","class_dict = {\n","    'GP': 0, 'G': 1, 'M': 2, 'T': 3, 'clear': 4 # выкинуть один класс - clear\n","}\n","\n","# class_dict = {\n","#     '-20':0, '-25':1, '-30':2, '-35':3\n","# }\n","\n","def get_class_from_path(path, class_dict=class_dict):\n","    for key in class_dict:\n","        if key in path:\n","            return class_dict[key]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.117458Z","iopub.status.busy":"2024-04-02T10:23:59.117157Z","iopub.status.idle":"2024-04-02T10:23:59.353857Z","shell.execute_reply":"2024-04-02T10:23:59.352941Z","shell.execute_reply.started":"2024-04-02T10:23:59.117433Z"},"trusted":true},"outputs":[],"source":["root_dir = '/kaggle/input/lozzzz/all_images'\n","image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n","\n","new_img_files = []\n","labels = []\n","for elem in image_files:\n","    for key in class_dict:\n","        if key in elem:\n","            labels.append(class_dict[key])\n","            new_img_files.append(elem)\n","            break\n","        else:\n","            continue"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.355297Z","iopub.status.busy":"2024-04-02T10:23:59.354981Z","iopub.status.idle":"2024-04-02T10:23:59.370085Z","shell.execute_reply":"2024-04-02T10:23:59.369122Z","shell.execute_reply.started":"2024-04-02T10:23:59.355271Z"},"trusted":true},"outputs":[],"source":["train_paths, valid_paths = train_test_split(new_img_files, random_state=42, shuffle=True, train_size=0.7, stratify=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.372080Z","iopub.status.busy":"2024-04-02T10:23:59.371434Z","iopub.status.idle":"2024-04-02T10:23:59.376635Z","shell.execute_reply":"2024-04-02T10:23:59.375605Z","shell.execute_reply.started":"2024-04-02T10:23:59.372055Z"},"trusted":true},"outputs":[],"source":["labels_train = []\n","for elem in train_paths:\n","    for key in class_dict:\n","        if key in elem:\n","            labels_train.append(class_dict[key])\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.378030Z","iopub.status.busy":"2024-04-02T10:23:59.377720Z","iopub.status.idle":"2024-04-02T10:23:59.473220Z","shell.execute_reply":"2024-04-02T10:23:59.472220Z","shell.execute_reply.started":"2024-04-02T10:23:59.378007Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","Counter(labels_train)\n","\n","class_counts = [106, 104, 89, 99, 59]\n","class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n","class_weights /= class_weights.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.474969Z","iopub.status.busy":"2024-04-02T10:23:59.474604Z","iopub.status.idle":"2024-04-02T10:23:59.622492Z","shell.execute_reply":"2024-04-02T10:23:59.621471Z","shell.execute_reply.started":"2024-04-02T10:23:59.474931Z"},"trusted":true},"outputs":[],"source":["@jit(nopython=True)\n","def cut_fragments(image, mode, n, size):\n","    height, width = image.shape[:2]\n","    fragments = []\n","    if mode == 'central':\n","        for i in range(n):\n","            for j in range(n):\n","                left = (width / n) * i\n","                upper = (height / n) * j\n","                right = left + size\n","                lower = upper + size\n","                fragment = image[int(upper):int(lower), int(left):int(right)]\n","                fragments.append(fragment)\n","    elif mode == 'random':\n","        for _ in range(n):\n","            left = random.randint(0, width - size)\n","            upper = random.randint(0, height - size)\n","            right = left + size\n","            lower = upper + size\n","            fragment = image[int(upper):int(lower), int(left):int(right)]\n","            fragments.append(fragment)\n","    return fragments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.626109Z","iopub.status.busy":"2024-04-02T10:23:59.625802Z","iopub.status.idle":"2024-04-02T10:23:59.637694Z","shell.execute_reply":"2024-04-02T10:23:59.636538Z","shell.execute_reply.started":"2024-04-02T10:23:59.626081Z"},"trusted":true},"outputs":[],"source":["class LozDataset(Dataset):\n","    def __init__(self, root_dir, image_files, mode: str = 'central', n: int = 3, size: int = 224, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_files = [os.path.join(self.root_dir, image_file) for image_file in image_files]\n","        self.mode = mode\n","        self.n = n\n","        self.size = size\n","        \n","    def __len__(self):\n","        if self.mode == 'central':  \n","            return len(self.image_files) * self.n * self.n\n","        else:\n","            return len(self.image_files) * self.n\n","\n","    def __getitem__(self, idx):\n","        if self.mode:\n","            fragments_per_image = self.n * self.n\n","        else:\n","            fragments_per_image = self.n\n","\n","        image_idx = idx // fragments_per_image\n","        fragment_idx = idx % fragments_per_image\n","        \n","        img_name = self.image_files[image_idx]\n","        image = Image.open(img_name).convert('L')\n","        image_np = np.array(image)\n","        \n","        fragments = cut_fragments(image=image_np, mode=self.mode, n=self.n, size=self.size)\n","        fragment = fragments[fragment_idx]\n","        \n","        # нормализуем фрагмент\n","#         max_value = np.max(fragment)\n","#         fragment = fragment / max_value\n","#         fragment = cut_percentiles(fragment)\n","#         fragment = apply_bilateral_filter_to_normalized(fragment)\n","        \n","        if self.transform:\n","            fragment = self.transform(image=fragment)['image']\n","        \n","        label = get_class_from_path(img_name)\n","        image_t = torch.tensor(fragment, dtype=torch.float32)\n","\n","        return image_t, img_name, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.639621Z","iopub.status.busy":"2024-04-02T10:23:59.638982Z","iopub.status.idle":"2024-04-02T10:23:59.652346Z","shell.execute_reply":"2024-04-02T10:23:59.651471Z","shell.execute_reply.started":"2024-04-02T10:23:59.639587Z"},"trusted":true},"outputs":[],"source":["# соль и перец\n","class SaltAndPepper(A.ImageOnlyTransform):\n","    def __init__(self, p=1., salt_ratio=0.5, amount=0.0008, always_apply=True):\n","        super().__init__(always_apply, p)\n","        self.salt_ratio = salt_ratio\n","        self.amount = amount\n","\n","    def apply(self, image, **params):\n","        image_copy = np.copy(image)  # создание копии изображения\n","\n","        num_salt = np.ceil(self.amount * image.size * self.salt_ratio)\n","        coords_salt = [np.random.randint(0, i - 1, int(num_salt)) for i in image_copy.shape]\n","        image_copy[coords_salt[0], coords_salt[1]] = 1\n","\n","        num_pepper = np.ceil(self.amount * image.size * (1.0 - self.salt_ratio))\n","        coords_pepper = [np.random.randint(0, i - 1, int(num_pepper)) for i in image_copy.shape]\n","        image_copy[coords_pepper[0], coords_pepper[1]] = 0\n","\n","        return image_copy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.654429Z","iopub.status.busy":"2024-04-02T10:23:59.653518Z","iopub.status.idle":"2024-04-02T10:23:59.666998Z","shell.execute_reply":"2024-04-02T10:23:59.666247Z","shell.execute_reply.started":"2024-04-02T10:23:59.654379Z"},"trusted":true},"outputs":[],"source":["train_transform = A.Compose([\n","    A.HorizontalFlip(p=.3),\n","#     A.RandomBrightnessContrast(p=1, contrast_limit=(.2), brightness_by_max=True, brightness_limit=(.2)),\n","    A.Rotate(limit=30, p=.3),\n","#     A.GaussianBlur(p=1, blur_limit=(1,3)),\n","#     A.CoarseDropout(max_holes=6, p=1., fill_value=200, max_height=3, max_width=3),\n","#     A.CoarseDropout(max_holes=6, p=1., fill_value=0, max_height=3, max_width=3),\n","#     A.GaussNoise(var_limit=(10.0), p=1), #белый шум\n","    SaltAndPepper(salt_ratio=0.4),\n","#     A.ElasticTransform(alpha=2, sigma=20, alpha_affine=10, p=.4),\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.668214Z","iopub.status.busy":"2024-04-02T10:23:59.667935Z","iopub.status.idle":"2024-04-02T10:23:59.678240Z","shell.execute_reply":"2024-04-02T10:23:59.677506Z","shell.execute_reply.started":"2024-04-02T10:23:59.668185Z"},"trusted":true},"outputs":[],"source":["root_dir = '/kaggle/input/lozzzz/all_images'\n","train_dataset = LozDataset(root_dir, train_paths, n=5, transform=train_transform)\n","valid_dataset = LozDataset(root_dir, valid_paths, n=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.679700Z","iopub.status.busy":"2024-04-02T10:23:59.679433Z","iopub.status.idle":"2024-04-02T10:23:59.688877Z","shell.execute_reply":"2024-04-02T10:23:59.688203Z","shell.execute_reply.started":"2024-04-02T10:23:59.679676Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, drop_last=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:23:59.690145Z","iopub.status.busy":"2024-04-02T10:23:59.689871Z","iopub.status.idle":"2024-04-02T10:23:59.725038Z","shell.execute_reply":"2024-04-02T10:23:59.724033Z","shell.execute_reply.started":"2024-04-02T10:23:59.690123Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import upload_file, HfFolder, notebook_login, hf_hub_download\n","from collections import OrderedDict\n","\n","class LozzModel():\n","    def __init__(self, model_name: str, kylberg_on: bool = False, num_classes: int = 5):\n","        self.model_name = model_name\n","        self.kylberg_on = kylberg_on\n","        self.num_classes = num_classes\n","        \n","    def load_model(self):\n","        \n","        if self.model_name == 'mobilenet-v2':\n","            model = models.mobilenet_v2(pretrained=True)\n","            model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            model.classifier[1] = nn.Linear(model.last_channel, self.num_classes)\n","            \n","        elif self.model_name == 'efficientnet-b0':\n","            model = models.efficientnet_b0(pretrained=True)\n","            model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            model.classifier[1] = nn.Linear(in_features=1280, out_features=self.num_classes, bias=True)\n","            \n","        elif self.model_name == 'efficientnet-b1':\n","            model = models.efficientnet_b1(pretrained=True)\n","            first_conv_layer = model.features[0][0]\n","            model.features[0][0] = torch.nn.Conv2d(1, first_conv_layer.out_channels, \n","                                      kernel_size=first_conv_layer.kernel_size, \n","                                      stride=first_conv_layer.stride, \n","                                      padding=first_conv_layer.padding, bias=False)\n","            model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, self.num_classes)\n","          \n","        elif self.model_name == 'efficientnet_v2_l':\n","            model = models.efficientnet_v2_l(weights='IMAGENET1K_V1')\n","            model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, self.num_classes, bias=True)\n","            \n","        elif self.model_name == 'resnet-50':\n","            model = models.resnet50(pretrained=True)\n","            model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","            model.fc = nn.Linear(in_features=2048, out_features=self.num_classes, bias=True)\n","            \n","        return model\n","    \n","    def load_kylberg_weights(self, repo_id: str, filename: str):\n","        \n","        weights_path = hf_hub_download(repo_id=repo_id, filename=filename)\n","        state_dict = torch.load(weights_path)\n","        \n","        if self.model_name == 'mobilenet-v2':\n","            model = models.mobilenet_v2(weights=None)\n","            model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            model.classifier[1] = nn.Linear(model.last_channel, 28)\n","            model.load_state_dict(state_dict)\n","            model.classifier[1] = nn.Linear(model.last_channel, self.num_classes)\n","           \n","        # ! TODO:\n","        # add other models\n","        \n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:24:08.425521Z","iopub.status.busy":"2024-04-02T10:24:08.424641Z","iopub.status.idle":"2024-04-02T10:24:09.468347Z","shell.execute_reply":"2024-04-02T10:24:09.467321Z","shell.execute_reply.started":"2024-04-02T10:24:08.425485Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","from huggingface_hub import upload_file, HfFolder, notebook_login, hf_hub_download\n","from collections import OrderedDict\n","\n","\n","NUM_CLASSES = len(class_dict)\n","\n","repo_id = 'danzzzll/mobilenet-v2-textures'\n","filename = 'mobilenet-v2-textures.pth'\n","\n","def loading_weights(repo_id, filename):\n","    weights_path = hf_hub_download(repo_id=repo_id, filename=filename)\n","    \n","    model = models.mobilenet_v2(weights=None)\n","    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    model.classifier[1] = nn.Linear(model.last_channel, 28)\n","\n","    state_dict = torch.load(weights_path, map_location='cpu')\n","    \n","    new_state_dict = OrderedDict()\n","    for k, v in state_dict.items():\n","        name = k[7:] if k.startswith('module.') else k  # remove `module.` prefix if exists\n","        new_state_dict[name] = v\n","\n","    model.load_state_dict(new_state_dict)\n","    model.classifier[1] = nn.Linear(model.last_channel, NUM_CLASSES)\n","\n","    if torch.cuda.device_count() > 1:\n","        model = torch.nn.DataParallel(model)\n","    \n","    model = model.to(device)\n","    return model\n","\n","model = loading_weights(repo_id, filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:24:13.938480Z","iopub.status.busy":"2024-04-02T10:24:13.937849Z","iopub.status.idle":"2024-04-02T10:24:13.945325Z","shell.execute_reply":"2024-04-02T10:24:13.944319Z","shell.execute_reply.started":"2024-04-02T10:24:13.938446Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n","# criterion = FocalLoss(alpha=.8)\n","\n","optimizer = torch.optim.AdamW(model.parameters())\n","# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.6)\n","\n","milestones = [12, 15, 26]\n","gamma = 0.3\n","exp_lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:24:28.616416Z","iopub.status.busy":"2024-04-02T10:24:28.616064Z","iopub.status.idle":"2024-04-02T10:25:03.211076Z","shell.execute_reply":"2024-04-02T10:25:03.210206Z","shell.execute_reply.started":"2024-04-02T10:24:28.616391Z"},"trusted":true},"outputs":[],"source":["import wandb\n","wandb.init(\n","    project=\"first_expirement\",\n","    name='experiment 16.3 - kylberg',\n","    config={\n","        \"architecture\": \"mobilenet\",\n","        \"dataset\": \"lozz\",\n","        \"epochs\": 30,\n","        \"fragments\": 25,\n","        \"central\": True,\n","        \"batch_size\": 16,\n","        \"classes\": 5,\n","        \"size\": 224,\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:25:09.324520Z","iopub.status.busy":"2024-04-02T10:25:09.324128Z","iopub.status.idle":"2024-04-02T10:25:09.344583Z","shell.execute_reply":"2024-04-02T10:25:09.343728Z","shell.execute_reply.started":"2024-04-02T10:25:09.324490Z"},"trusted":true},"outputs":[],"source":["train_loss = []\n","train_acc = []\n","train_full = []\n","test_loss = []\n","test_acc = []\n","test_full = []\n","def train_and_validate(epoch, model):\n","                                                ### train\n","    print(f'EPOCH: {epoch + 1}')\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    fragments_train = MultiDict()\n","    model.train()\n","    for batch_idx, (data, name, target) in tqdm(enumerate(train_dataloader)):\n","        target = target.type(torch.LongTensor).to(device)\n","        data = data.unsqueeze(1)\n","        data, target = data.to(device).float(), target.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criterion(outputs, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        preds = outputs.argmax(dim=1)\n","        running_acc += (preds == target).float().mean().item()\n","        \n","        # quality on full images\n","        for n, pred in zip(name, preds):\n","            fragments_train.add(n, pred.item())\n","        \n","#         if batch_idx % 150 == 0:\n","#             random_img = data.cpu().numpy()[np.random.randint(data.size(0))][0]\n","#             plt.imshow(random_img, cmap='gray')\n","#             plt.title(\"Random Image from Batch\")\n","#             plt.axis('off')\n","#             plt.show()\n","    \n","    result_train = most_common_class_per_key(fragments_train)\n","    train_full.append(accuracy_full(result_train))\n","    train_loss.append(running_loss / len(train_dataloader))\n","    train_acc.append(running_acc / len(train_dataloader))\n","    \n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss[-1]:.3f}, Train Acc: {train_acc[-1]:.3f}, Train Full Images Acc: {train_full[-1]:.3f}\")\n","    exp_lr_scheduler.step()\n","    \n","                                                ### validate\n","\n","    model.eval()\n","    all_preds = [] \n","    all_targets = [] \n","    fragments_test = MultiDict()\n","    with torch.no_grad():\n","        running_acc = 0.0\n","        for batch_idx, (data, name, target) in enumerate(valid_dataloader):\n","            target = target.type(torch.LongTensor).to(device)\n","            data = data.unsqueeze(1)\n","            data, target = data.to(device).float(), target.to(device)\n","\n","            outputs = model(data)\n","            loss = criterion(outputs, target)\n","            running_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            running_acc += (preds == target).float().mean().item()\n","\n","            all_targets.extend(target.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","            \n","            # quality on full images\n","            for n, pred in zip(name, preds):\n","                fragments_test.add(n, pred.item())\n","    \n","    result_test = most_common_class_per_key(fragments_test)\n","    test_full.append(accuracy_full(result_test))\n","    test_loss.append(running_loss / len(valid_dataloader))\n","    test_acc.append(running_acc / len(valid_dataloader))\n","    \n","    print(f\"Epoch {epoch+1}, Valid Loss: {test_loss[-1]:.3f}, Valid Acc: {test_acc[-1]:.3f}, Valid Full Images Acc: {test_full[-1]:.3f}\")\n","    \n","    cm = confusion_matrix(all_targets, all_preds)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","    disp.plot()\n","    plt.show()\n","    \n","    wandb.log({\n","        \"train_acc\": train_acc[-1],\n","        \"train_loss\": train_loss[-1],\n","        \"train_full\": train_full[-1] / 100,\n","        \"valid_acc\": test_acc[-1],\n","        \"valid_loss\": test_loss[-1],\n","        \"valid_full\": test_full[-1] / 100\n","    })\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:25:10.065935Z","iopub.status.busy":"2024-04-02T10:25:10.065562Z","iopub.status.idle":"2024-04-02T10:25:10.078034Z","shell.execute_reply":"2024-04-02T10:25:10.076976Z","shell.execute_reply.started":"2024-04-02T10:25:10.065904Z"},"trusted":true},"outputs":[],"source":["def get_predictions(model=model):\n","    \"\"\"\n","    возвращает MultiDict, в котором каждому названию картинки соответствует несколько значений\n","    это предсказания для фрагментов данного изображения\n","    для каждого изображения будет строчек столько, на сколько фрагментов разбиваем это изображение\n","    \"\"\"\n","    fragments = MultiDict()\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (data, name, target) in enumerate(valid_dataloader):\n","            data = data.unsqueeze(1)\n","            data, target = data.to(device), target.to(device)\n","            data = data.float()\n","            outputs = model.to(device)(data)\n","            preds = outputs.argmax(dim=1)\n","            \n","            for n, pred in zip(name, preds):\n","                fragments.add(n, pred.item())\n","    return fragments\n","\n","from collections import Counter\n","\n","def most_common_class_per_key(multidict):\n","    \"\"\"\n","    Получает MultiDict на вход и подсчитывает для одного изображения самый частый предсказанный класс\n","    Выдает словарь, с названием изображения и самым частым классом\n","    \"\"\"\n","    result = {}\n","    keys = set(multidict.keys())\n","    \n","    for key in keys:\n","        values = multidict.getall(key)\n","        count = Counter(values)  \n","        most_common_class, _ = count.most_common(1)[0]\n","        result[key] = most_common_class\n","        \n","    return result\n","\n","def accuracy_full(result: dict):  \n","    \"\"\"\n","    result - словарь, где каждому пути к изображению сопоставляется самый часто встречаемый класс\n","    return accuracy - между предсказанными значениями и истинными\n","    \"\"\"\n","    true_val = 0.0\n","    for key, value in result.items():\n","        y_true = get_class_from_path(f\"'{key}'\")\n","        y_pred = result[key]\n","        if y_true == y_pred:\n","            true_val += 1\n","    accuracy = round(true_val / len(result) * 100, 3)\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:25:10.991017Z","iopub.status.busy":"2024-04-02T10:25:10.990652Z","iopub.status.idle":"2024-04-02T12:44:39.299394Z","shell.execute_reply":"2024-04-02T12:44:39.297617Z","shell.execute_reply.started":"2024-04-02T10:25:10.990983Z"},"trusted":true},"outputs":[],"source":["best_loss = float('inf')\n","epochs_without_improvement = 0\n","early_stopping_threshold = 5\n","\n","for epoch in range(30):\n","    model = train_and_validate(epoch, model)\n","    \n","    if test_loss[-1] < best_loss:\n","        best_loss = test_loss[-1]\n","        best_weights = model.state_dict()\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","        \n","    if epochs_without_improvement >= early_stopping_threshold:\n","        print(\"Early stopping triggered after {} epochs without improvement.\".format(epochs_without_improvement))\n","        break\n","model.load_state_dict(best_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:26:31.041896Z","iopub.status.busy":"2024-04-02T09:26:31.040963Z","iopub.status.idle":"2024-04-02T09:26:34.974118Z","shell.execute_reply":"2024-04-02T09:26:34.973409Z","shell.execute_reply.started":"2024-04-02T09:26:31.041860Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["## Оценка результатов для полных картинок"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:25:46.400784Z","iopub.status.busy":"2024-04-02T09:25:46.400412Z","iopub.status.idle":"2024-04-02T09:26:21.623807Z","shell.execute_reply":"2024-04-02T09:26:21.622664Z","shell.execute_reply.started":"2024-04-02T09:25:46.400757Z"},"trusted":true},"outputs":[],"source":["fragments = get_predictions()\n","result = most_common_class_per_key(fragments)\n","print(f\"Accuracy для полных картинок: {accuracy_full(result)}%\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3400341,"sourceId":6236972,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
