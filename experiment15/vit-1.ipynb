{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:02:40.966463Z","iopub.status.busy":"2023-11-12T08:02:40.966179Z","iopub.status.idle":"2023-11-12T08:02:54.202392Z","shell.execute_reply":"2023-11-12T08:02:54.201336Z","shell.execute_reply.started":"2023-11-12T08:02:40.966437Z"},"trusted":true},"outputs":[],"source":["!pip install multidict -q #efficientnet_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:02:54.205393Z","iopub.status.busy":"2023-11-12T08:02:54.204707Z","iopub.status.idle":"2023-11-12T08:03:00.676771Z","shell.execute_reply":"2023-11-12T08:03:00.675827Z","shell.execute_reply.started":"2023-11-12T08:02:54.205352Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torchvision.models as models\n","\n","from numba import jit\n","\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm.auto import tqdm\n","from multidict import MultiDict\n","\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:00.678432Z","iopub.status.busy":"2023-11-12T08:03:00.678011Z","iopub.status.idle":"2023-11-12T08:03:00.688285Z","shell.execute_reply":"2023-11-12T08:03:00.687355Z","shell.execute_reply.started":"2023-11-12T08:03:00.678404Z"},"trusted":true},"outputs":[],"source":["DEFAULT_RANDOM_SEED = 42\n","import random\n","import numpy as np\n","\n","\n","def set_all_seeds(seed=DEFAULT_RANDOM_SEED):\n","\n","    # python's seeds\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","    # torch's seeds\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_all_seeds(seed=DEFAULT_RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:00.690734Z","iopub.status.busy":"2023-11-12T08:03:00.690422Z","iopub.status.idle":"2023-11-12T08:03:00.703147Z","shell.execute_reply":"2023-11-12T08:03:00.701998Z","shell.execute_reply.started":"2023-11-12T08:03:00.690708Z"},"trusted":true},"outputs":[],"source":["# для получения label для каждого файла\n","class_dict = {\n","    'GP': 0, 'G': 1, 'M': 2, 'T': 3, 'clear': 4 # выкинуть один класс - clear\n","}\n","\n","# class_dict = {\n","#     '-20':0, '-25':1, '-30':2, '-35':3\n","# }\n","\n","def get_class_from_path(path, class_dict=class_dict):\n","    for key in class_dict:\n","        if key in path:\n","            return class_dict[key]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:00.704824Z","iopub.status.busy":"2023-11-12T08:03:00.704408Z","iopub.status.idle":"2023-11-12T08:03:00.914826Z","shell.execute_reply":"2023-11-12T08:03:00.913868Z","shell.execute_reply.started":"2023-11-12T08:03:00.704792Z"},"trusted":true},"outputs":[],"source":["root_dir = '/kaggle/input/lozzzz/all_images'\n","image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n","\n","new_img_files = []\n","labels = []\n","for elem in image_files:\n","    for key in class_dict:\n","        if key in elem:\n","            labels.append(class_dict[key])\n","            new_img_files.append(elem)\n","            break\n","        else:\n","            continue"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:00.916134Z","iopub.status.busy":"2023-11-12T08:03:00.915862Z","iopub.status.idle":"2023-11-12T08:03:00.926134Z","shell.execute_reply":"2023-11-12T08:03:00.925201Z","shell.execute_reply.started":"2023-11-12T08:03:00.916110Z"},"trusted":true},"outputs":[],"source":["train_paths, valid_paths = train_test_split(new_img_files, random_state=42, shuffle=True, train_size=0.7, stratify=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:00.928155Z","iopub.status.busy":"2023-11-12T08:03:00.927747Z","iopub.status.idle":"2023-11-12T08:03:00.935935Z","shell.execute_reply":"2023-11-12T08:03:00.934907Z","shell.execute_reply.started":"2023-11-12T08:03:00.928068Z"},"trusted":true},"outputs":[],"source":["labels_train = []\n","for elem in train_paths:\n","    for key in class_dict:\n","        if key in elem:\n","            labels_train.append(class_dict[key])\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:00.937792Z","iopub.status.busy":"2023-11-12T08:03:00.937203Z","iopub.status.idle":"2023-11-12T08:03:01.005766Z","shell.execute_reply":"2023-11-12T08:03:01.004908Z","shell.execute_reply.started":"2023-11-12T08:03:00.937729Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","Counter(labels_train)\n","\n","class_counts = [106, 104, 89, 99, 59]\n","class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n","class_weights /= class_weights.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:01.007671Z","iopub.status.busy":"2023-11-12T08:03:01.007003Z","iopub.status.idle":"2023-11-12T08:03:01.017520Z","shell.execute_reply":"2023-11-12T08:03:01.016758Z","shell.execute_reply.started":"2023-11-12T08:03:01.007637Z"},"trusted":true},"outputs":[],"source":["\n","def cut_fragments(image, mode, n, size):\n","    height, width = image.shape[:2]\n","    fragments = []\n","    if mode == 'central':\n","        for i in range(n):\n","            for j in range(n):\n","                left = (width / n) * i\n","                upper = (height / n) * j\n","                right = left + size\n","                lower = upper + size\n","                fragment = image[int(upper):int(lower), int(left):int(right)]\n","                fragments.append(fragment)\n","    elif mode == 'random':\n","        for _ in range(n):\n","            left = random.randint(0, width - size)\n","            upper = random.randint(0, height - size)\n","            right = left + size\n","            lower = upper + size\n","            fragment = image[int(upper):int(lower), int(left):int(right)]\n","            fragments.append(fragment)\n","    return fragments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:01.021058Z","iopub.status.busy":"2023-11-12T08:03:01.020758Z","iopub.status.idle":"2023-11-12T08:03:01.028614Z","shell.execute_reply":"2023-11-12T08:03:01.027700Z","shell.execute_reply.started":"2023-11-12T08:03:01.021031Z"},"trusted":true},"outputs":[],"source":["# обрезаем хвосты и заменяем их на 0 и 1\n","def cut_percentiles(image):\n","    q1 = np.percentile(image, 1)\n","    q99 = np.percentile(image, 99)\n","\n","    image[image < q1] = 0\n","    image[image > q99] = 1\n","    return image\n","\n","def bilateral_filter(image):\n","    return cv2.bilateralFilter(image, 9, 75, 75)\n","\n","# применение bilateral filter\n","def apply_bilateral_filter_to_normalized(image):\n","    image_8bit = (image * 255).astype(np.uint8)\n","    return bilateral_filter(image_8bit) / 255.0\n","\n","def median_filter(image):\n","    return cv2.medianBlur(image, 5)\n","\n","def apply_median_filter_to_normalized(image):\n","    image_8bit = (image * 255).astype(np.uint8)\n","    return median_filter(image_8bit) / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:01.030265Z","iopub.status.busy":"2023-11-12T08:03:01.029981Z","iopub.status.idle":"2023-11-12T08:03:01.042185Z","shell.execute_reply":"2023-11-12T08:03:01.041230Z","shell.execute_reply.started":"2023-11-12T08:03:01.030241Z"},"trusted":true},"outputs":[],"source":["class LozDataset(Dataset):\n","    def __init__(self, root_dir, image_files, feature_extractor, mode: str = 'central', n: int = 3, size: int = 224, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_files = [os.path.join(self.root_dir, image_file) for image_file in image_files]\n","        self.mode = mode\n","        self.n = n\n","        self.size = size\n","        self.feature_extractor = feature_extractor\n","        \n","    def __len__(self):\n","        if self.mode == 'central':  \n","            return len(self.image_files) * self.n * self.n\n","        else:\n","            return len(self.image_files) * self.n\n","\n","    def __getitem__(self, idx):\n","        if self.mode:\n","            fragments_per_image = self.n * self.n\n","        else:\n","            fragments_per_image = self.n\n","\n","        image_idx = idx // fragments_per_image\n","        fragment_idx = idx % fragments_per_image\n","        \n","        img_name = self.image_files[image_idx]\n","        image = Image.open(img_name).convert('L')\n","        image_np = np.array(image)\n","        \n","        fragments = cut_fragments(image=image_np, mode=self.mode, n=self.n, size=self.size)\n","        fragment = fragments[fragment_idx]\n","        \n","        # нормализуем фрагмент\n","        max_value = np.max(fragment)\n","        fragment = fragment / max_value\n","#         fragment = cut_percentiles(fragment)\n","#         fragment = apply_bilateral_filter_to_normalized(fragment)\n","        \n","        if self.transform:\n","            fragment = self.transform(image=fragment)['image']\n","        \n","        label = get_class_from_path(img_name)\n","        image_t = torch.tensor(fragment, dtype=torch.float32)\n","        image_t = image_t[:, :, None]\n","        image_t = self.feature_extractor(images=image_t, return_tensors=\"pt\", do_rescale=False).pixel_values[0]\n","        \n","        return image_t, img_name, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:01.043560Z","iopub.status.busy":"2023-11-12T08:03:01.043310Z","iopub.status.idle":"2023-11-12T08:03:01.054784Z","shell.execute_reply":"2023-11-12T08:03:01.053923Z","shell.execute_reply.started":"2023-11-12T08:03:01.043538Z"},"trusted":true},"outputs":[],"source":["# соль и перец\n","class SaltAndPepper(A.ImageOnlyTransform):\n","    def __init__(self, p=1., salt_ratio=0.5, amount=0.0008, always_apply=True):\n","        super().__init__(always_apply, p)\n","        self.salt_ratio = salt_ratio\n","        self.amount = amount\n","\n","    def apply(self, image, **params):\n","        image_copy = np.copy(image)  # создание копии изображения\n","\n","        num_salt = np.ceil(self.amount * image.size * self.salt_ratio)\n","        coords_salt = [np.random.randint(0, i - 1, int(num_salt)) for i in image_copy.shape]\n","        image_copy[coords_salt[0], coords_salt[1]] = 1\n","\n","        num_pepper = np.ceil(self.amount * image.size * (1.0 - self.salt_ratio))\n","        coords_pepper = [np.random.randint(0, i - 1, int(num_pepper)) for i in image_copy.shape]\n","        image_copy[coords_pepper[0], coords_pepper[1]] = 0\n","\n","        return image_copy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:01.056214Z","iopub.status.busy":"2023-11-12T08:03:01.055946Z","iopub.status.idle":"2023-11-12T08:03:01.069360Z","shell.execute_reply":"2023-11-12T08:03:01.068478Z","shell.execute_reply.started":"2023-11-12T08:03:01.056191Z"},"trusted":true},"outputs":[],"source":["train_transform = A.Compose([\n","    A.HorizontalFlip(p=.3),\n","#     A.RandomBrightnessContrast(p=1, contrast_limit=(.2), brightness_by_max=True, brightness_limit=(.2)),\n","    A.Rotate(limit=30, p=.3),\n","#     A.GaussianBlur(p=1, blur_limit=(1,3)),\n","#     A.CoarseDropout(max_holes=6, p=1., fill_value=200, max_height=3, max_width=3),\n","#     A.CoarseDropout(max_holes=6, p=1., fill_value=0, max_height=3, max_width=3),\n","#     A.GaussNoise(var_limit=(10.0), p=1), #белый шум\n","    SaltAndPepper(salt_ratio=0.4),\n","#     A.ElasticTransform(alpha=2, sigma=20, alpha_affine=10, p=.4),\n","])\n"]},{"cell_type":"markdown","metadata":{},"source":["# try to use VIT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:01.070816Z","iopub.status.busy":"2023-11-12T08:03:01.070458Z","iopub.status.idle":"2023-11-12T08:03:14.113236Z","shell.execute_reply":"2023-11-12T08:03:14.112209Z","shell.execute_reply.started":"2023-11-12T08:03:01.070787Z"},"trusted":true},"outputs":[],"source":["from transformers import MobileViTForImageClassification, MobileViTFeatureExtractor"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:14.115008Z","iopub.status.busy":"2023-11-12T08:03:14.114467Z","iopub.status.idle":"2023-11-12T08:03:15.652643Z","shell.execute_reply":"2023-11-12T08:03:15.651834Z","shell.execute_reply.started":"2023-11-12T08:03:14.114980Z"},"trusted":true},"outputs":[],"source":["model_name = \"apple/mobilevit-small\"\n","feature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-small\")\n","model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n","\n","# Assuming model is an instance of MobileViTForImageClassification\n","first_conv_layer = model.mobilevit.conv_stem.convolution\n","model.mobilevit.conv_stem.convolution = nn.Conv2d(1, first_conv_layer.out_channels, \n","                                                  kernel_size=first_conv_layer.kernel_size, \n","                                                  stride=first_conv_layer.stride, \n","                                                  padding=first_conv_layer.padding, \n","                                                  bias=False)\n","model.classifier = nn.Linear(in_features=640, out_features=5, bias=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:15.656360Z","iopub.status.busy":"2023-11-12T08:03:15.655801Z","iopub.status.idle":"2023-11-12T08:03:15.662782Z","shell.execute_reply":"2023-11-12T08:03:15.661900Z","shell.execute_reply.started":"2023-11-12T08:03:15.656305Z"},"trusted":true},"outputs":[],"source":["root_dir = '/kaggle/input/lozzzz/all_images'\n","train_dataset = LozDataset(root_dir, train_paths, feature_extractor, transform=train_transform, size=300, n=5)\n","valid_dataset = LozDataset(root_dir, valid_paths, feature_extractor, size=300, n=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:15.664762Z","iopub.status.busy":"2023-11-12T08:03:15.664401Z","iopub.status.idle":"2023-11-12T08:03:15.682909Z","shell.execute_reply":"2023-11-12T08:03:15.682006Z","shell.execute_reply.started":"2023-11-12T08:03:15.664729Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:15.684937Z","iopub.status.busy":"2023-11-12T08:03:15.684202Z","iopub.status.idle":"2023-11-12T08:03:21.076554Z","shell.execute_reply":"2023-11-12T08:03:21.075771Z","shell.execute_reply.started":"2023-11-12T08:03:15.684910Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","if torch.cuda.device_count() > 1:\n","    model = torch.nn.DataParallel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:21.077870Z","iopub.status.busy":"2023-11-12T08:03:21.077583Z","iopub.status.idle":"2023-11-12T08:03:21.085476Z","shell.execute_reply":"2023-11-12T08:03:21.084508Z","shell.execute_reply.started":"2023-11-12T08:03:21.077846Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n","optimizer = torch.optim.AdamW(model.parameters())\n","\n","milestones = [12, 15, 26]\n","gamma = 0.3\n","exp_lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:21.087106Z","iopub.status.busy":"2023-11-12T08:03:21.086729Z","iopub.status.idle":"2023-11-12T08:03:56.404769Z","shell.execute_reply":"2023-11-12T08:03:56.403767Z","shell.execute_reply.started":"2023-11-12T08:03:21.087080Z"},"trusted":true},"outputs":[],"source":["import wandb\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"first_expirement\",\n","    \n","    # track hyperparameters and run metadata\n","    config={\n","        \"architecture\": \"apple/mobilevit-small\",\n","        \"dataset\": \"lozz\",\n","        \"epochs\": 30,\n","        \"fragments\": 25,\n","        \"central\": True,\n","        \"batch_size\": 16,\n","        \"classes\": 5,\n","        \"size\": 300,\n","        \"preprocessing\": \"normalizing\",\n","        \"augmentations\": \"\"\"\n","                        A.HorizontalFlip(p=.3),\n","                    #     A.RandomBrightnessContrast(p=1, contrast_limit=(.2), brightness_by_max=True, brightness_limit=(.2)),\n","                        A.Rotate(limit=30, p=.3),\n","                    #     A.GaussianBlur(p=1, blur_limit=(1,3)),\n","                    #     A.CoarseDropout(max_holes=6, p=1., fill_value=200, max_height=3, max_width=3),\n","                    #     A.CoarseDropout(max_holes=6, p=1., fill_value=0, max_height=3, max_width=3),\n","                    #     A.GaussNoise(var_limit=(10.0), p=1), #белый шум\n","                        SaltAndPepper(salt_ratio=.4),\n","                    #    A.ElasticTransform(alpha=2, sigma=20, alpha_affine=10, p=.4),\n","                        \"\"\",\n","        \"optimizer\": 'torch.optim.AdamW(model.parameters())',\n","        \"criterion\": 'nn.CrossEntropyLoss(weight=class_weights.to(device))',\n","        \"sheduler\": 'torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)'\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:03:56.407474Z","iopub.status.busy":"2023-11-12T08:03:56.407204Z","iopub.status.idle":"2023-11-12T08:03:56.426633Z","shell.execute_reply":"2023-11-12T08:03:56.425702Z","shell.execute_reply.started":"2023-11-12T08:03:56.407449Z"},"trusted":true},"outputs":[],"source":["train_loss = []\n","train_acc = []\n","train_full = []\n","test_loss = []\n","test_acc = []\n","test_full = []\n","def train_and_validate(epoch, model):\n","                                                ### train\n","    print(f'EPOCH: {epoch + 1}')\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    fragments_train = MultiDict()\n","    model.train()\n","    for batch_idx, (data, name, target) in tqdm(enumerate(train_dataloader)):\n","        target = target.type(torch.LongTensor).to(device)\n","        data = data.squeeze(2).float().to(device)\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criterion(outputs.logits, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        preds = outputs.logits.argmax(dim=1)\n","        running_acc += (preds == target).float().mean().item()\n","        \n","        # quality on full images\n","        for n, pred in zip(name, preds):\n","            fragments_train.add(n, pred.item())\n","        \n","        if batch_idx % 150 == 0:\n","            random_img = data.cpu().numpy()[np.random.randint(data.size(0))][0]\n","            plt.imshow(random_img, cmap='gray')\n","            plt.title(\"Random Image from Batch\")\n","            plt.axis('off')\n","            plt.show()\n","    \n","    result_train = most_common_class_per_key(fragments_train)\n","    train_full.append(accuracy_full(result_train))\n","    train_loss.append(running_loss / len(train_dataloader))\n","    train_acc.append(running_acc / len(train_dataloader))\n","    \n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss[-1]:.3f}, Train Acc: {train_acc[-1]:.3f}, Train Full Images Acc: {train_full[-1]:.3f}\")\n","    exp_lr_scheduler.step()\n","    \n","                                                ### validate\n","\n","    model.eval()\n","    all_preds = [] \n","    all_targets = [] \n","    fragments_test = MultiDict()\n","    with torch.no_grad():\n","        running_acc = 0.0\n","        for batch_idx, (data, name, target) in enumerate(valid_dataloader):\n","            target = target.type(torch.LongTensor).to(device)\n","            data = data.squeeze(2).float().to(device)\n","\n","            outputs = model(data)\n","            loss = criterion(outputs.logits, target)\n","            running_loss += loss.item()\n","            preds = outputs.logits.argmax(dim=1)\n","            running_acc += (preds == target).float().mean().item()\n","\n","            all_targets.extend(target.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","            \n","            # quality on full images\n","            for n, pred in zip(name, preds):\n","                fragments_test.add(n, pred.item())\n","    \n","    result_test = most_common_class_per_key(fragments_test)\n","    test_full.append(accuracy_full(result_test))\n","    test_loss.append(running_loss / len(valid_dataloader))\n","    test_acc.append(running_acc / len(valid_dataloader))\n","    \n","    print(f\"Epoch {epoch+1}, Valid Loss: {test_loss[-1]:.3f}, Valid Acc: {test_acc[-1]:.3f}, Valid Full Images Acc: {test_full[-1]:.3f}\")\n","    \n","    cm = confusion_matrix(all_targets, all_preds)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","    disp.plot()\n","    plt.show()\n","    \n","    wandb.log({\n","        \"train_acc\": train_acc[-1],\n","        \"train_loss\": train_loss[-1],\n","        \"train_full\": train_full[-1] / 100,\n","        \"valid_acc\": test_acc[-1],\n","        \"valid_loss\": test_loss[-1],\n","        \"valid_full\": test_full[-1] / 100\n","    })\n","    \n","    return train_loss, train_acc, train_full, test_loss, test_acc, test_full"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:04:13.341372Z","iopub.status.busy":"2023-11-12T08:04:13.340992Z","iopub.status.idle":"2023-11-12T10:13:30.185451Z","shell.execute_reply":"2023-11-12T10:13:30.184140Z","shell.execute_reply.started":"2023-11-12T08:04:13.341340Z"},"trusted":true},"outputs":[],"source":["best_loss = float('inf')\n","epochs_without_improvement = 0\n","early_stopping_threshold = 5\n","\n","for epoch in range(20):\n","    train_loss, train_acc, train_full, test_loss, test_acc, test_full = train_and_validate(epoch, model)\n","    \n","    if test_loss[-1] < best_loss:\n","        best_loss = test_loss[-1]\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","        \n","    if epochs_without_improvement >= early_stopping_threshold:\n","        print(\"Early stopping triggered after {} epochs without improvement.\".format(epochs_without_improvement))\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T10:14:12.150014Z","iopub.status.busy":"2023-11-12T10:14:12.149353Z","iopub.status.idle":"2023-11-12T10:14:15.369365Z","shell.execute_reply":"2023-11-12T10:14:15.368613Z","shell.execute_reply.started":"2023-11-12T10:14:12.149975Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["## Оценка результатов для полных картинок"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:04:08.137985Z","iopub.status.busy":"2023-11-12T08:04:08.137254Z","iopub.status.idle":"2023-11-12T08:04:08.146306Z","shell.execute_reply":"2023-11-12T08:04:08.145372Z","shell.execute_reply.started":"2023-11-12T08:04:08.137949Z"},"trusted":true},"outputs":[],"source":["def get_predictions(model=model):\n","    \"\"\"\n","    возвращает MultiDict, в котором каждому названию картинки соответствует несколько значений\n","    это предсказания для фрагментов данного изображения\n","    для каждого изображения будет строчек столько, на сколько фрагментов разбиваем это изображение\n","    \"\"\"\n","    fragments = MultiDict()\n","    mobilenet_v2_model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (data, name, target) in enumerate(valid_dataloader):\n","            data = data.unsqueeze(1)\n","            data, target = data.to(device), target.to(device)\n","            data = data.float()\n","            outputs = mobilenet_v2_model.to(device)(data)\n","            preds = outputs.argmax(dim=1)\n","            \n","            for n, pred in zip(name, preds):\n","                fragments.add(n, pred.item())\n","    return fragments\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:04:08.563153Z","iopub.status.busy":"2023-11-12T08:04:08.562331Z","iopub.status.idle":"2023-11-12T08:04:08.571044Z","shell.execute_reply":"2023-11-12T08:04:08.570153Z","shell.execute_reply.started":"2023-11-12T08:04:08.563111Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","\n","def most_common_class_per_key(multidict):\n","    \"\"\"\n","    Получает MultiDict на вход и подсчитывает для одного изображения самый частый предсказанный класс\n","    Выдает словарь, с названием изображения и самым частым классом\n","    \"\"\"\n","    result = {}\n","    keys = set(multidict.keys())\n","    \n","    for key in keys:\n","        values = multidict.getall(key)\n","        count = Counter(values)  \n","        most_common_class, _ = count.most_common(1)[0]\n","        result[key] = most_common_class\n","        \n","    return result\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T08:04:09.076818Z","iopub.status.busy":"2023-11-12T08:04:09.075885Z","iopub.status.idle":"2023-11-12T08:04:09.084795Z","shell.execute_reply":"2023-11-12T08:04:09.083810Z","shell.execute_reply.started":"2023-11-12T08:04:09.076775Z"},"trusted":true},"outputs":[],"source":["def accuracy_full(result: dict):  \n","    \"\"\"\n","    result - словарь, где каждому пути к изображению сопоставляется самый часто встречаемый класс\n","    return accuracy - между предсказанными значениями и истинными\n","    \"\"\"\n","    true_val = 0.0\n","    for key, value in result.items():\n","        y_true = get_class_from_path(f\"'{key}'\")\n","        y_pred = result[key]\n","        if y_true == y_pred:\n","            true_val += 1\n","    accuracy = round(true_val / len(result) * 100, 3)\n","    return accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fragments = get_predictions()\n","result = most_common_class_per_key(fragments)\n","print(f\"Accuracy для полных картинок: {accuracy_full(result)}%\""]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3400341,"sourceId":6236972,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
