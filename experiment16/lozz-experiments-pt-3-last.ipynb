{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:23:49.736752Z","iopub.status.busy":"2024-04-02T08:23:49.735867Z","iopub.status.idle":"2024-04-02T08:24:03.941399Z","shell.execute_reply":"2024-04-02T08:24:03.940211Z","shell.execute_reply.started":"2024-04-02T08:23:49.736719Z"},"trusted":true},"outputs":[],"source":["!pip install multidict -q #efficientnet_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:03.943780Z","iopub.status.busy":"2024-04-02T08:24:03.943455Z","iopub.status.idle":"2024-04-02T08:24:20.023936Z","shell.execute_reply":"2024-04-02T08:24:20.023166Z","shell.execute_reply.started":"2024-04-02T08:24:03.943751Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# from efficientnet_pytorch import EfficientNet\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torchvision.models as models\n","\n","from numba import jit\n","\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm.auto import tqdm\n","from multidict import MultiDict\n","\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.025435Z","iopub.status.busy":"2024-04-02T08:24:20.025011Z","iopub.status.idle":"2024-04-02T08:24:20.034997Z","shell.execute_reply":"2024-04-02T08:24:20.034185Z","shell.execute_reply.started":"2024-04-02T08:24:20.025411Z"},"trusted":true},"outputs":[],"source":["DEFAULT_RANDOM_SEED = 42\n","import random\n","import numpy as np\n","\n","\n","def set_all_seeds(seed=DEFAULT_RANDOM_SEED):\n","\n","    # python's seeds\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","    # torch's seeds\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_all_seeds(seed=DEFAULT_RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.037333Z","iopub.status.busy":"2024-04-02T08:24:20.037080Z","iopub.status.idle":"2024-04-02T08:24:20.044533Z","shell.execute_reply":"2024-04-02T08:24:20.043731Z","shell.execute_reply.started":"2024-04-02T08:24:20.037311Z"},"trusted":true},"outputs":[],"source":["# для получения label для каждого файла\n","class_dict = {\n","    'GP': 0, 'G': 1, 'M': 2, 'T': 3, 'clear': 4 # выкинуть один класс - clear\n","}\n","\n","# class_dict = {\n","#     '-20':0, '-25':1, '-30':2, '-35':3\n","# }\n","\n","def get_class_from_path(path, class_dict=class_dict):\n","    for key in class_dict:\n","        if key in path:\n","            return class_dict[key]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.045735Z","iopub.status.busy":"2024-04-02T08:24:20.045448Z","iopub.status.idle":"2024-04-02T08:24:20.164023Z","shell.execute_reply":"2024-04-02T08:24:20.163198Z","shell.execute_reply.started":"2024-04-02T08:24:20.045677Z"},"trusted":true},"outputs":[],"source":["root_dir = '/kaggle/input/lozzzz/all_images'\n","image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n","\n","new_img_files = []\n","labels = []\n","for elem in image_files:\n","    for key in class_dict:\n","        if key in elem:\n","            labels.append(class_dict[key])\n","            new_img_files.append(elem)\n","            break\n","        else:\n","            continue"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.165227Z","iopub.status.busy":"2024-04-02T08:24:20.165007Z","iopub.status.idle":"2024-04-02T08:24:20.177462Z","shell.execute_reply":"2024-04-02T08:24:20.176740Z","shell.execute_reply.started":"2024-04-02T08:24:20.165208Z"},"trusted":true},"outputs":[],"source":["train_paths, valid_paths = train_test_split(new_img_files, random_state=42, shuffle=True, train_size=0.7, stratify=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.178810Z","iopub.status.busy":"2024-04-02T08:24:20.178483Z","iopub.status.idle":"2024-04-02T08:24:20.185942Z","shell.execute_reply":"2024-04-02T08:24:20.185261Z","shell.execute_reply.started":"2024-04-02T08:24:20.178775Z"},"trusted":true},"outputs":[],"source":["labels_train = []\n","for elem in train_paths:\n","    for key in class_dict:\n","        if key in elem:\n","            labels_train.append(class_dict[key])\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.187248Z","iopub.status.busy":"2024-04-02T08:24:20.186957Z","iopub.status.idle":"2024-04-02T08:24:20.284614Z","shell.execute_reply":"2024-04-02T08:24:20.283977Z","shell.execute_reply.started":"2024-04-02T08:24:20.187216Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","Counter(labels_train)\n","\n","class_counts = [106, 104, 89, 99, 59]\n","class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n","class_weights /= class_weights.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.285815Z","iopub.status.busy":"2024-04-02T08:24:20.285550Z","iopub.status.idle":"2024-04-02T08:24:20.443835Z","shell.execute_reply":"2024-04-02T08:24:20.443141Z","shell.execute_reply.started":"2024-04-02T08:24:20.285793Z"},"trusted":true},"outputs":[],"source":["@jit(nopython=True)\n","def cut_fragments(image, mode, n, size):\n","    height, width = image.shape[:2]\n","    fragments = []\n","    if mode == 'central':\n","        for i in range(n):\n","            for j in range(n):\n","                left = (width / n) * i\n","                upper = (height / n) * j\n","                right = left + size\n","                lower = upper + size\n","                fragment = image[int(upper):int(lower), int(left):int(right)]\n","                fragments.append(fragment)\n","    elif mode == 'random':\n","        for _ in range(n):\n","            left = random.randint(0, width - size)\n","            upper = random.randint(0, height - size)\n","            right = left + size\n","            lower = upper + size\n","            fragment = image[int(upper):int(lower), int(left):int(right)]\n","            fragments.append(fragment)\n","    return fragments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.447548Z","iopub.status.busy":"2024-04-02T08:24:20.447296Z","iopub.status.idle":"2024-04-02T08:24:20.458067Z","shell.execute_reply":"2024-04-02T08:24:20.457207Z","shell.execute_reply.started":"2024-04-02T08:24:20.447528Z"},"trusted":true},"outputs":[],"source":["class LozDataset(Dataset):\n","    def __init__(self, root_dir, image_files, mode: str = 'central', n: int = 3, size: int = 224, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_files = [os.path.join(self.root_dir, image_file) for image_file in image_files]\n","        self.mode = mode\n","        self.n = n\n","        self.size = size\n","        \n","    def __len__(self):\n","        if self.mode == 'central':  \n","            return len(self.image_files) * self.n * self.n\n","        else:\n","            return len(self.image_files) * self.n\n","\n","    def __getitem__(self, idx):\n","        if self.mode:\n","            fragments_per_image = self.n * self.n\n","        else:\n","            fragments_per_image = self.n\n","\n","        image_idx = idx // fragments_per_image\n","        fragment_idx = idx % fragments_per_image\n","        \n","        img_name = self.image_files[image_idx]\n","        image = Image.open(img_name).convert('L')\n","        image_np = np.array(image)\n","        \n","        fragments = cut_fragments(image=image_np, mode=self.mode, n=self.n, size=self.size)\n","        fragment = fragments[fragment_idx]\n","        \n","        # нормализуем фрагмент\n","#         max_value = np.max(fragment)\n","#         fragment = fragment / max_value\n","#         fragment = cut_percentiles(fragment)\n","#         fragment = apply_bilateral_filter_to_normalized(fragment)\n","        \n","        if self.transform:\n","            fragment = self.transform(image=fragment)['image']\n","        \n","        label = get_class_from_path(img_name)\n","        image_t = torch.tensor(fragment, dtype=torch.float32)\n","\n","        return image_t, img_name, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.459583Z","iopub.status.busy":"2024-04-02T08:24:20.459230Z","iopub.status.idle":"2024-04-02T08:24:20.475117Z","shell.execute_reply":"2024-04-02T08:24:20.474404Z","shell.execute_reply.started":"2024-04-02T08:24:20.459552Z"},"trusted":true},"outputs":[],"source":["# соль и перец\n","class SaltAndPepper(A.ImageOnlyTransform):\n","    def __init__(self, p=1., salt_ratio=0.5, amount=0.0008, always_apply=True):\n","        super().__init__(always_apply, p)\n","        self.salt_ratio = salt_ratio\n","        self.amount = amount\n","\n","    def apply(self, image, **params):\n","        image_copy = np.copy(image)  # создание копии изображения\n","\n","        num_salt = np.ceil(self.amount * image.size * self.salt_ratio)\n","        coords_salt = [np.random.randint(0, i - 1, int(num_salt)) for i in image_copy.shape]\n","        image_copy[coords_salt[0], coords_salt[1]] = 1\n","\n","        num_pepper = np.ceil(self.amount * image.size * (1.0 - self.salt_ratio))\n","        coords_pepper = [np.random.randint(0, i - 1, int(num_pepper)) for i in image_copy.shape]\n","        image_copy[coords_pepper[0], coords_pepper[1]] = 0\n","\n","        return image_copy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.476213Z","iopub.status.busy":"2024-04-02T08:24:20.475951Z","iopub.status.idle":"2024-04-02T08:24:20.491595Z","shell.execute_reply":"2024-04-02T08:24:20.490940Z","shell.execute_reply.started":"2024-04-02T08:24:20.476190Z"},"trusted":true},"outputs":[],"source":["train_transform = A.Compose([\n","    A.HorizontalFlip(p=.3),\n","#     A.RandomBrightnessContrast(p=1, contrast_limit=(.2), brightness_by_max=True, brightness_limit=(.2)),\n","    A.Rotate(limit=30, p=.3),\n","#     A.GaussianBlur(p=1, blur_limit=(1,3)),\n","#     A.CoarseDropout(max_holes=6, p=1., fill_value=200, max_height=3, max_width=3),\n","#     A.CoarseDropout(max_holes=6, p=1., fill_value=0, max_height=3, max_width=3),\n","#     A.GaussNoise(var_limit=(10.0), p=1), #белый шум\n","    SaltAndPepper(salt_ratio=0.4),\n","#     A.ElasticTransform(alpha=2, sigma=20, alpha_affine=10, p=.4),\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.492868Z","iopub.status.busy":"2024-04-02T08:24:20.492604Z","iopub.status.idle":"2024-04-02T08:24:20.505627Z","shell.execute_reply":"2024-04-02T08:24:20.504963Z","shell.execute_reply.started":"2024-04-02T08:24:20.492847Z"},"trusted":true},"outputs":[],"source":["root_dir = '/kaggle/input/lozzzz/all_images'\n","train_dataset = LozDataset(root_dir, train_paths, n=3, transform=train_transform)\n","valid_dataset = LozDataset(root_dir, valid_paths, n=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.507336Z","iopub.status.busy":"2024-04-02T08:24:20.506937Z","iopub.status.idle":"2024-04-02T08:24:20.517920Z","shell.execute_reply":"2024-04-02T08:24:20.517235Z","shell.execute_reply.started":"2024-04-02T08:24:20.507307Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, drop_last=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.519084Z","iopub.status.busy":"2024-04-02T08:24:20.518857Z","iopub.status.idle":"2024-04-02T08:24:20.555865Z","shell.execute_reply":"2024-04-02T08:24:20.554977Z","shell.execute_reply.started":"2024-04-02T08:24:20.519065Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:24:20.557388Z","iopub.status.busy":"2024-04-02T08:24:20.557079Z","iopub.status.idle":"2024-04-02T08:24:21.524186Z","shell.execute_reply":"2024-04-02T08:24:21.523387Z","shell.execute_reply.started":"2024-04-02T08:24:20.557361Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","from huggingface_hub import upload_file, HfFolder, notebook_login, hf_hub_download\n","from collections import OrderedDict\n","\n","\n","NUM_CLASSES = len(class_dict)\n","\n","repo_id = 'danzzzll/mobilenet-v2-textures'\n","filename = 'mobilenet-v2-textures.pth'\n","\n","def loading_weights(repo_id, filename):\n","    weights_path = hf_hub_download(repo_id=repo_id, filename=filename)\n","    \n","    model = models.mobilenet_v2(weights=None)\n","    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    model.classifier[1] = nn.Linear(model.last_channel, 28)\n","\n","    state_dict = torch.load(weights_path, map_location='cpu')\n","    \n","    new_state_dict = OrderedDict()\n","    for k, v in state_dict.items():\n","        name = k[7:] if k.startswith('module.') else k  # remove `module.` prefix if exists\n","        new_state_dict[name] = v\n","\n","    model.load_state_dict(new_state_dict)\n","    model.classifier[1] = nn.Linear(model.last_channel, NUM_CLASSES)\n","\n","    if torch.cuda.device_count() > 1:\n","        model = torch.nn.DataParallel(model)\n","    \n","    model = model.to(device)\n","    return model\n","\n","model = loading_weights(repo_id, filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:25:02.070258Z","iopub.status.busy":"2024-04-02T08:25:02.069388Z","iopub.status.idle":"2024-04-02T08:25:02.076702Z","shell.execute_reply":"2024-04-02T08:25:02.075851Z","shell.execute_reply.started":"2024-04-02T08:25:02.070223Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n","# criterion = FocalLoss(alpha=.8)\n","\n","optimizer = torch.optim.AdamW(model.parameters())\n","# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.6)\n","\n","milestones = [12, 15, 26]\n","gamma = 0.3\n","exp_lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:25:05.281950Z","iopub.status.busy":"2024-04-02T08:25:05.281571Z","iopub.status.idle":"2024-04-02T08:25:39.745336Z","shell.execute_reply":"2024-04-02T08:25:39.744512Z","shell.execute_reply.started":"2024-04-02T08:25:05.281921Z"},"trusted":true},"outputs":[],"source":["import wandb\n","wandb.init(\n","    project=\"first_expirement\",\n","    name='experiment 16.1 - kylberg',\n","    config={\n","        \"architecture\": \"mobilenet\",\n","        \"dataset\": \"lozz\",\n","        \"epochs\": 30,\n","        \"fragments\": 9,\n","        \"central\": True,\n","        \"batch_size\": 16,\n","        \"classes\": 5,\n","        \"size\": 224,\n","        \"optimizer\": 'torch.optim.AdamW(mobilenet_v2_model.parameters())',\n","        \"criterion\": 'nn.CrossEntropyLoss(weight=class_weights.to(device))',\n","        \"sheduler\": 'torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)'\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T08:25:49.012090Z","iopub.status.busy":"2024-04-02T08:25:49.011613Z","iopub.status.idle":"2024-04-02T08:25:49.031885Z","shell.execute_reply":"2024-04-02T08:25:49.031044Z","shell.execute_reply.started":"2024-04-02T08:25:49.012041Z"},"trusted":true},"outputs":[],"source":["train_loss = []\n","train_acc = []\n","train_full = []\n","test_loss = []\n","test_acc = []\n","test_full = []\n","def train_and_validate(epoch, model):\n","                                                ### train\n","    print(f'EPOCH: {epoch + 1}')\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    fragments_train = MultiDict()\n","    model.train()\n","    for batch_idx, (data, name, target) in tqdm(enumerate(train_dataloader)):\n","        target = target.type(torch.LongTensor).to(device)\n","        data = data.unsqueeze(1)\n","        data, target = data.to(device).float(), target.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criterion(outputs, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        preds = outputs.argmax(dim=1)\n","        running_acc += (preds == target).float().mean().item()\n","        \n","        # quality on full images\n","        for n, pred in zip(name, preds):\n","            fragments_train.add(n, pred.item())\n","        \n","#         if batch_idx % 150 == 0:\n","#             random_img = data.cpu().numpy()[np.random.randint(data.size(0))][0]\n","#             plt.imshow(random_img, cmap='gray')\n","#             plt.title(\"Random Image from Batch\")\n","#             plt.axis('off')\n","#             plt.show()\n","    \n","    result_train = most_common_class_per_key(fragments_train)\n","    train_full.append(accuracy_full(result_train))\n","    train_loss.append(running_loss / len(train_dataloader))\n","    train_acc.append(running_acc / len(train_dataloader))\n","    \n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss[-1]:.3f}, Train Acc: {train_acc[-1]:.3f}, Train Full Images Acc: {train_full[-1]:.3f}\")\n","    exp_lr_scheduler.step()\n","    \n","                                                ### validate\n","\n","    model.eval()\n","    all_preds = [] \n","    all_targets = [] \n","    fragments_test = MultiDict()\n","    with torch.no_grad():\n","        running_acc = 0.0\n","        for batch_idx, (data, name, target) in enumerate(valid_dataloader):\n","            target = target.type(torch.LongTensor).to(device)\n","            data = data.unsqueeze(1)\n","            data, target = data.to(device).float(), target.to(device)\n","\n","            outputs = model(data)\n","            loss = criterion(outputs, target)\n","            running_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            running_acc += (preds == target).float().mean().item()\n","\n","            all_targets.extend(target.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","            \n","            # quality on full images\n","            for n, pred in zip(name, preds):\n","                fragments_test.add(n, pred.item())\n","    \n","    result_test = most_common_class_per_key(fragments_test)\n","    test_full.append(accuracy_full(result_test))\n","    test_loss.append(running_loss / len(valid_dataloader))\n","    test_acc.append(running_acc / len(valid_dataloader))\n","    \n","    print(f\"Epoch {epoch+1}, Valid Loss: {test_loss[-1]:.3f}, Valid Acc: {test_acc[-1]:.3f}, Valid Full Images Acc: {test_full[-1]:.3f}\")\n","    \n","    cm = confusion_matrix(all_targets, all_preds)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","    disp.plot()\n","    plt.show()\n","    \n","    wandb.log({\n","        \"train_acc\": train_acc[-1],\n","        \"train_loss\": train_loss[-1],\n","        \"train_full\": train_full[-1] / 100,\n","        \"valid_acc\": test_acc[-1],\n","        \"valid_loss\": test_loss[-1],\n","        \"valid_full\": test_full[-1] / 100\n","    })\n","    \n","    return train_loss, train_acc, test_loss, test_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:25:44.008278Z","iopub.status.busy":"2024-04-02T09:25:44.007841Z","iopub.status.idle":"2024-04-02T09:25:44.023308Z","shell.execute_reply":"2024-04-02T09:25:44.022139Z","shell.execute_reply.started":"2024-04-02T09:25:44.008236Z"},"trusted":true},"outputs":[],"source":["def get_predictions(model=model):\n","    \"\"\"\n","    возвращает MultiDict, в котором каждому названию картинки соответствует несколько значений\n","    это предсказания для фрагментов данного изображения\n","    для каждого изображения будет строчек столько, на сколько фрагментов разбиваем это изображение\n","    \"\"\"\n","    fragments = MultiDict()\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (data, name, target) in enumerate(valid_dataloader):\n","            data = data.unsqueeze(1)\n","            data, target = data.to(device), target.to(device)\n","            data = data.float()\n","            outputs = model.to(device)(data)\n","            preds = outputs.argmax(dim=1)\n","            \n","            for n, pred in zip(name, preds):\n","                fragments.add(n, pred.item())\n","    return fragments\n","\n","from collections import Counter\n","\n","def most_common_class_per_key(multidict):\n","    \"\"\"\n","    Получает MultiDict на вход и подсчитывает для одного изображения самый частый предсказанный класс\n","    Выдает словарь, с названием изображения и самым частым классом\n","    \"\"\"\n","    result = {}\n","    keys = set(multidict.keys())\n","    \n","    for key in keys:\n","        values = multidict.getall(key)\n","        count = Counter(values)  \n","        most_common_class, _ = count.most_common(1)[0]\n","        result[key] = most_common_class\n","        \n","    return result\n","\n","def accuracy_full(result: dict):  \n","    \"\"\"\n","    result - словарь, где каждому пути к изображению сопоставляется самый часто встречаемый класс\n","    return accuracy - между предсказанными значениями и истинными\n","    \"\"\"\n","    true_val = 0.0\n","    for key, value in result.items():\n","        y_true = get_class_from_path(f\"'{key}'\")\n","        y_pred = result[key]\n","        if y_true == y_pred:\n","            true_val += 1\n","    accuracy = round(true_val / len(result) * 100, 3)\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T08:25:50.761461Z","iopub.status.busy":"2024-04-02T08:25:50.760434Z","iopub.status.idle":"2024-04-02T09:22:49.775890Z","shell.execute_reply":"2024-04-02T09:22:49.774461Z","shell.execute_reply.started":"2024-04-02T08:25:50.761420Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["best_loss = float('inf')\n","epochs_without_improvement = 0\n","early_stopping_threshold = 5\n","\n","for epoch in range(30):\n","    train_loss, train_acc, test_loss, test_acc = train_and_validate(epoch, model)\n","    \n","    if test_loss[-1] < best_loss:\n","        best_loss = test_loss[-1]\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","        \n","    if epochs_without_improvement >= early_stopping_threshold:\n","        print(\"Early stopping triggered after {} epochs without improvement.\".format(epochs_without_improvement))\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:26:31.041896Z","iopub.status.busy":"2024-04-02T09:26:31.040963Z","iopub.status.idle":"2024-04-02T09:26:34.974118Z","shell.execute_reply":"2024-04-02T09:26:34.973409Z","shell.execute_reply.started":"2024-04-02T09:26:31.041860Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["## Оценка результатов для полных картинок"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:25:46.400784Z","iopub.status.busy":"2024-04-02T09:25:46.400412Z","iopub.status.idle":"2024-04-02T09:26:21.623807Z","shell.execute_reply":"2024-04-02T09:26:21.622664Z","shell.execute_reply.started":"2024-04-02T09:25:46.400757Z"},"trusted":true},"outputs":[],"source":["fragments = get_predictions()\n","result = most_common_class_per_key(fragments)\n","print(f\"Accuracy для полных картинок: {accuracy_full(result)}%\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3400341,"sourceId":6236972,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
